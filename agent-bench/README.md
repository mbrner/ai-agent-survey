<!--- Created using: gpt-4 --->
<!--- Reviewed: False --->
# AgentBench: Evaluating LLMs as Agents

**Link**: [Paper](http://arxiv.org/pdf/2308.03688v1)

**Authors**: Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, Jie Tang

## Summary

**TL;DR: This paper introduces AgentBench, a comprehensive benchmark for evaluating Large Language Models (LLMs) as agents across diverse real-world tasks, revealing a significant performance gap between top-tier and open-source models, and providing a toolkit for future research and development in the field.**

Large Language Models (LLMs) such as GPT-4 have shown impressive abilities in understanding human intent and executing instructions, leading to the development of applications like AutoGPT, BabyAGI, and AgentGPT. However, the absence of a systematic and standard benchmark to evaluate LLM-as-Agent presents a significant challenge. This paper introduces AgentBench, a multi-dimensional evolving benchmark that currently includes 8 distinct environments to assess the reasoning and decision-making abilities of LLM-as-Agent in a multi-turn open-ended generation setting. These environments include a simulated online shopping environment, Webshop, which tests the agent's ability to search, view, and choose items on a real e-commerce website. While games have been used as simulated environments for intelligent agent development, this paper focuses on text-based games, specifically a digital card game, as an ideal option for text-only LLM evaluation.

### Approach

AgentBench is the first systematic benchmark to evaluate LLM-as-Agent across a wide array of real-world challenges and 8 distinct environments. These environments include tasks such as lateral thinking puzzles, household web browsing, operating system tasks, and database interactions. A significant addition is the ALFWorld benchmark, which mimics household scenarios and requires the agent to break down complex high-level targets into a sequence of straightforward actions. The agent receives environment feedback after each step, allowing it to adapt the plan dynamically. The benchmark also includes a digital card game (DCG) environment, adapted from a simplified DCG system—Aquawar—from the 2021 Tsinghua University Agent Competition. In Aquawar, the agent acts as a player managing a team of fishes with different talents to battle against another team in a turn-based form, testing the model's understanding of game rules, operating logic, and strategic decision-making abilities. The game rules are simplified and include four types of pet fish, each with unique active and passive skills. The Webshop environment simulates an online shopping experience, with the agent interacting with a database of about a million products scraped from amazon.com. The evaluation setup employs a 1-shot evaluation setting, with the model's output assessed using the BLEU metric for similarity to valid action options. The evaluation process is divided into two parts: Initialization, where the task is described to the model along with a successful example, and Interaction, where the model generates thoughts and actions based on feedback and environment information. The Webshop environment also includes a reward function that maps the similarity of the attributes of the expected and actual bought product to a number between 0 and 1. The AgentBench design carefully balances evaluation comprehensiveness and efficiency, and the toolkit is designed to interact only with APIs, simplifying the process for LLMs that want to test on AgentBench.
### Results

The study extensively tests over 25 LLMs (including APIs and open-sourced models) and finds a significant disparity in performance between top commercial LLMs and open-sourced competitors. Top-tier models like GPT-4 are capable of handling a wide array of real-world tasks, including the digital card game environment and the Webshop online shopping simulation, indicating the potential for developing a potent, continuously learning agent. However, there is a significant performance gap between these top-tier models and their open-source counterparts. The performance of open-source LLMs on AgentBench tasks lags considerably, underscoring the need for additional efforts to enhance the learning abilities of open-source LLMs. The overall success rate, defined as the number of tasks successfully completed by the model divided by the total number of tasks, is used as a measure of model performance. The evaluation also includes metrics such as Element Accuracy, Action F1, and Step Success Rate. The latter is reported as the main metric due to the current struggles for LLMs to ensure overall task success rates. The overall score is calculated by averaging the scores of each task across all the models, scaling them to an average of 1. This method ensures fairness and consistency in evaluation, enabling easier comparisons and analysis in future research.
## Conclusion

AgentBench provides a comprehensive and systematic benchmark for evaluating LLMs as agents across diverse real-world challenges. It addresses the limitations of existing benchmarks and provides a more accurate reflection of the practical use-cases of LLMs. It serves as an ideal testbed for both LLM and agent evaluation, and reveals a significant performance disparity between top-tier and open-source models. The benchmark is continuously evolving to cover more scenarios and tasks in LLM-as-agent evaluation, with plans for further refinement and enrichment in its next version. The integrated toolkit, along with the associated datasets and environments, are made publicly available for the broader research community. The AgentBench design and toolkit, which is API-oriented and uses docker images for complex environments, simplifies the process for researchers and LLMs that want to test on AgentBench. The study also identifies specific challenges for LLMs in agent-formed tasks, including action validity, long context, and multi-turn consistency. The impact of code training is also considered. The performance of open-sourced LLMs, in particular, is significantly lower than that of API-based LLMs, with the most capable open-sourced model, openchat-13b-v3.2, still presenting a clear performance gap compared to gpt-3.5-turbo. This highlights the need for further research and development in the field of open-source LLMs.
