<!--- Created using: gpt-4 --->
<!--- Reviewed: False --->
# Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents

**Link**: [Paper](http://arxiv.org/pdf/2302.01560v1)

**Authors**: Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, Yitao Liang

## Summary

**TL;DR: The paper presents 'Describe, Explain, Plan and Select' (DEPS), a novel planning approach for multi-task embodied agents in Minecraft, which incorporates error correction, proximity-based goal selection, and Large Language Models (LLMs), achieving significant improvements in task success rates and demonstrating potential for application in other open-world environments.**

The paper addresses the challenge of planning in Minecraft, an open-world environment for developing multi-task embodied agents. The authors identify two main challenges: the need for precise, multi-step reasoning due to the long-term nature of tasks, and the inefficiency of vanilla planners that do not consider the agent's current proximity when ordering parallel sub-goals. These challenges bottleneck planning-based agents in open-world environments. The development of such agents is seen as a key milestone towards generally capable artificial intelligence, and the authors note the success of a hierarchical goal execution architecture in various robotics domains.

### Approach

The authors propose 'Describe, Explain, Plan and Select' (DEPS), an interactive planning approach based on Large Language Models (LLMs). DEPS incorporates error correction from feedback during long-term planning and introduces a sense of proximity through a goal Selector. The Selector is a state-aware module that ranks parallel sub-goals based on estimated completion steps, improving the original plan. It predicts the goal distribution under the current state and plan, choosing the nearest goal as the current goal from the candidate goal sets in the plan. This approach improves the efficiency of the plans and increases the final success rate. The DEPS approach also includes a descriptor that summarizes the current situation as text and sends it back to the LLM-based planner when a failure occurs. The LLM-based planner then acts as an explainer to locate the errors in the previous plan and re-plan the task to obtain a correct plan. The Selector was further refined to consider practical task experience and the difficulty of completing the goal, using a horizon-predictive selector that estimates the remaining time steps to complete a given goal. During the training of the controller, the authors adopted the observation space provided by MineDoJo, which includes an RGB camera view, yaw/pitch angle, GPS location, and the type of 3 Ã— 3 blocks surrounding the agent. The original multi-discrete action space provided by MineDoJo was discretized into 42 discrete actions. A modified goal-sensitive Impala CNN was used as the backbone network in the proposed imitation learning method. Additionally, the authors introduced a Goal Mapping and a Prompt Generator. The Goal Mapping maps the plan expressed in free-form language to the pre-defined controller skills set using regular expressions and semantic distance calculations. The Prompt Generator translates task instructions into prompt text, adding examples and demonstrations to make the LLM output familiar to the chain-of-thought thinking and structural output.
### Results

The DEPS approach has been tested on 71 tasks in Minecraft, distilled from the 1572 programmatic tasks of Minedojo, grouped into 8 different meta groups, each focusing on testing a different aspect of the proposed method. These tasks, known as TASK101, represent the hardest conditions and require multiple reasoning steps for successful completion. The DEPS system achieved significant milestones, including the ability to robustly accomplish these tasks, nearly doubling the overall success rate. The complete DEPS system ultimately yields an 88.44% gain in average success rate. The planning success rate is computed considering an oracle and perfect controller if provided. Notably, DEPS is the first agent with planning that ever achieves an above-zero success rate on the challenging ObtainDiamond task. The DEPS approach was compared with other LLM-based planners, including Zero-shot Planner, Prog-Prompt, Chain of Thought, Inner Monologue, and Code as Policies. These methods were adapted to conform to the Minecraft specification based on prompt and feedback template design. The DEPS approach outperforms all other methods across all meta tasks, even as task complexity increases. The success rates of DEPS under different maximum rounds of interactive feedback were also evaluated, showing consistent improvement as the number of rounds increased. Additional tests were conducted to compare the performance of DEPS with different selector implementations, including using a fixed sequence of goals, a random sequence of goals, and selecting a goal based on MineCLIP, CLIP, and the horizon-predictive Selector. The horizon-predictive Selector showed significant improvement over other methods. Furthermore, the DEPS approach demonstrated state-of-the-art performance on almost all tasks, especially on difficult tasks that require long reasoning steps, as shown in the detailed success rate table of all tasks.
## Conclusion

The paper concludes with an exploration of how the DEPS design outperforms counterparts and provides a promising update on the ObtainDiamond grand challenge. The authors also discuss the potential for transferring the success of this approach to more open-ended worlds like Minecraft. The code for this project has been made publicly available. The authors also highlight the robustness of the DEPS approach, which consistently outperforms other LLM-based planners across all meta tasks, validating the effectiveness of the 'describe, explain and plan' components in estimating and correcting plan failures. Ablation studies further confirmed the effectiveness of the DEPS rounds and the horizon-predictive Selector in improving the execution efficiency of the plan in embodied environments. The DEPS planner is zero-shot, making it possible to generalize to other long-horizon open worlds. This is a significant departure from previous efforts that focused on improving the low-level controller, and it significantly influences the complexity and breadth of tasks that the agent can handle. However, the authors acknowledge two major limitations: the reliance on privately-held LLMs like GPT-3 and ChatGPT, and the explicit planning in the system which can prevent the model from being further scaled up. Future work will explore using open-sourced models and integrating the planning within an end-to-end trainable goal-conditioned policy.
