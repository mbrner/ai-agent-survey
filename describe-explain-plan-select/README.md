<!--- Created using: gpt-4 --->
<!--- Reviewed: False --->
# Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents

**Link**: [Paper](http://arxiv.org/pdf/2302.01560v1)

**Authors**: Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, Yitao Liang

## Summary

**TL;DR: The paper introduces 'Describe, Explain, Plan and Select' (DEPS), a novel planning approach for multi-task agents in Minecraft that incorporates error correction, proximity-based goal selection, and precise object identification, resulting in an 88.44% increase in success rate and the first-ever positive success rate in the challenging ObtainDiamond task.**

The paper addresses the problem of planning in Minecraft, a complex, open-ended environment for developing multi-task embodied agents. The authors identify two main challenges: the need for precise, multi-step reasoning due to the long-term nature of tasks, and the inefficiency of traditional planners that do not consider the agent's current proximity when ordering parallel sub-goals. These challenges significantly degrade the success rate as the number of reasoning steps required by the task increases.

### Approach

The authors propose 'Describe, Explain, Plan and Select' (DEPS), an interactive planning approach based on Large Language Models (LLMs). DEPS incorporates error correction from feedback during long-term planning and introduces a sense of proximity through a goal Selector. This learnable module ranks parallel sub-goals based on estimated completion steps and modifies the original plan accordingly, making it more efficient. The DEPS system also takes into account the exact object name and quantities required for each sub-goal, ensuring that subsequent sub-goals can be executed successfully. The DEPS approach also includes a descriptor that summarizes the current situation as text and sends it back to the LLM-based planner when a failure occurs. The planner then acts as an explainer to locate the errors in the previous plan and re-plan the task to obtain a correct plan. Unlike previous methods, DEPS does not assume the initial plan from the LLM is correct, and can locate and correct errors in previous plans. The controller used in the DEPS system was trained using the imitation learning method proposed by Cai et al. (2023), with a modified goal-sensitive Impala CNN as the backbone network. Additionally, the DEPS system includes a Goal Mapping component that maps the plan expressed in free-form language to the pre-defined controller skills set, and a Prompt Generator that translates task instructions into prompt text.
### Results

The experiments demonstrate the first multi-task agent capable of robustly accomplishing over 70 Minecraft tasks, nearly doubling overall performance. The authors also provide detailed ablation and exploratory studies to show how their design outperforms counterparts. The planning success rate is computed considering an oracle and perfect controller if provided. The DEPS system ultimately yields an 88.44% gain in average success rate. Notably, DEPS is the first agent with planning that ever achieves an above-zero success rate on the challenging ObtainDiamond task. The DEPS method was compared with other LLM-based planners, including Zero-shot Planner, Prog-Prompt, Chain of Thought, Inner Monologue, and Code as Policies. These methods were adapted to conform to the Minecraft specification based on prompt and feedback template design. All planner methods accessed the LLM model through the OpenAI API. The experiments were standardized with a controller learned by behavior cloning to minimize performance variation. The authors introduced a testing environment, Minecraft Task101, consisting of the hardest 71 tasks distilled from Minedojo. As the complexity of the task increases, the success rate of all agents decreases with the reasoning steps increasing. Starting from a certain complexity level, almost all existing LLM-based planners fail, but DEPS consistently outperforms them. Additional experiments showed that the Selector module significantly improved the final task success rate, with the horizon-predictive Selector (HPS) yielding the best performance.
## Conclusion

The paper concludes with a promising update on the ObtainDiamond grand challenge using the DEPS approach. The authors have released their code for further exploration and use. The contributions of the paper are twofold: proposing the DEPS method to address the main issues in open-world planning, including multi-step reasoning and the lack of sense of agentâ€™s proximity to the sub-goals, and demonstrating its effectiveness in a complex environment like Minecraft. The paper also acknowledges the limitations of the LLM-based planner, which can lead to non-executable and redundant plans, and the difficulty of executing the initial plan in one shot due to potential errors. However, these issues are addressed through the DEPS system's error correction and feedback mechanisms. The paper also highlights the significant improvement brought about by the Selector module, especially the horizon-predictive Selector, in efficiency-sensitive tasks. The DEPS approach is unique in its emphasis on applying domain knowledge to propose and arrange sub-goals, significantly influencing the complexity and breadth of tasks that the agent can handle. Moreover, the DEPS planner is zero-shot, making it possible to generalize to other long-horizon open worlds. Despite the impressive results, the authors acknowledge two major limitations: the reliance on privately-held LLMs like GPT-3 and ChatGPT, which may limit accessibility, and the explicit planning in the system, which could prevent further scaling up. Future work will explore using open-sourced models and integrating planning within an end-to-end trainable goal-conditioned policy.
