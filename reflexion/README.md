<!--- Created using: ... --->
<!--- Based on: 100.0% of the Paper --->
<!--- Reviewed: False --->
# Self-planning Code Generation with Large Language Models

**Relevance: ...**

**Link**:
- Paper: [Arxiv](http://arxiv.org/pdf/2303.06689v2)
- Code: N/A

**Authors**: Xue Jiang, Yihong Dong, Lecheng Wang, Zheng Fang, Qiwei Shang, Ge Li, Zhi Jin, Wenpin Jiao

## Summary

**TL;DR: This paper proposes a self-planning approach for large language models (LLMs) to generate complex code, demonstrating through extensive experiments that this method significantly improves code quality and performance, simplifies problem-solving, and outperforms other methods, even with the limitation of manual crafting of prompts.**

The paper explores the challenges large language models (LLMs) encounter when generating complex code that aligns with human intent. It emphasizes the human strategy of using planning to decompose complex problems and schedule solution steps, suggesting this approach be incorporated into code generation. The paper also underscores the importance of programming as a problem-solving tool and the central role of code generation in programming theory.

### Approach

The authors propose a self-planning code generation approach with LLMs, divided into two phases: planning and implementation. The planning phase involves the LLM abstracting and decomposing the intent to create a plan for guiding code generation, aided by few-shot prompting techniques like Chain of Thought (CoT). The self-planning prompt is appended before the intent, guiding the LLM to perform planning. The plan is a scheduling of subproblems abstracted and decomposed from the intent. During inference, the test-time intent is concatenated after the prompt, and the LLM attempts to do planning for the test-time intent. The implementation phase involves the model generating code step by step, guided by the preceding solution steps. The plan obtained during the planning phase is appended to the intent as input for the LLM, which generates the final code by predicting the next token. The authors also suggest that the sequential list used to represent plans could be converted into a hierarchical list to better handle complex problem domains.

### Results

The authors conducted extensive experiments on various code-generation benchmarks across multiple programming languages including Python, Java, Go, and JavaScript. They used datasets like HumanEval, HumanEval-X, MBPP-ET, and HumanEval-ET for evaluation. The results demonstrate that self-planning code generation improves Pass@1 by up to 25.4% compared to direct code generation. Additionally, the self-planning approach enhances the quality of the generated code in terms of correctness, readability, and robustness, as evaluated by humans. Empirical evaluations have provided evidence that self-planning approach can substantially improve the performance of LLMs on code generation. Self-planning is an emergent ability that appears on large enough LLMs. The authors also compared the self-planning approach with other baselines, including CoT and direct code generation, and found that self-planning consistently outperforms these methods across different datasets and evaluation metrics.

### Conclusion

The paper concludes that integrating planning into code generation significantly enhances the performance and quality of code generated by LLMs. This approach simplifies complex problems, provides a problem abstraction that guides the model in code generation, and ultimately results in more correct, readable, and robust code. The authors also highlight the potential of few-shot prompting techniques in implementing planning without the need for fine-tuning. They further explore several variants of the self-planning approach and confirm that their designed self-planning approach is the optimal choice among these variants. The study also suggests that besides increasing model size, incorporating code training data and RLHF can also enhance a model's self-planning capabilities. The self-planning approach reduces the complexity of code generation by providing guidance at key points of code generation, as evidenced by the increased probability of key tokens in the generated code. Through human evaluation, the self-planning approach exhibits the best readability among all approaches, and its correctness and robustness performance is on par with the ground-truth planning approach. The self-planning approach can thoroughly consider some edge cases and determines the legality of inputs, as evidenced by the qualitative examples. The human evaluation conducted with 10 developers further validates the superiority of the self-planning approach in terms of correctness, readability, and robustness compared to other methods. Despite the limitation of manual crafting of prompts, the self-planning approach demonstrates improved data efficiency, requiring only 8 examples for training, and can be crafted by people without programming knowledge. This makes the self-planning code generation approach easier to apply in practice. The authors also suggest that it may be worthwhile to explore beyond code writing to the realm of requirements analysis, incorporating the methodology of requirements engineering with LLMs.

## Implications/Learnings for GPT4Hana

...
