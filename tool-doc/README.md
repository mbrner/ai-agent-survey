<!--- Created using: gpt-4 --->
<!--- Reviewed: False --->
# Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models

**Link**: [Paper](http://arxiv.org/pdf/2308.00675v1)

**Authors**: Cheng-Yu Hsieh, Si-An Chen, Chun-Liang Li, Yasuhisa Fujii, Alexander Ratner, Chen-Yu Lee, Ranjay Krishna, Tomas Pfister

## Summary

**TL;DR: This study demonstrates that using tool documentation, rather than demonstrations, can effectively teach large language models (LLMs) new tools, simplifying the process, reducing bias, and improving scalability, with the performance of LLMs even improving with the comprehensiveness of the documentation, up to a certain length.**

The study explores the potential of using tool documentation, akin to a craftsman's manual, as a method for teaching large language models (LLMs) new tools. The authors argue that demonstrations, while effective, can be difficult to obtain, may lead to biased usage if not carefully chosen, and lack a systematic approach for selection. This issue becomes more complex as the number of tools and tasks increase. The paper likens an LLM to a craftsman, breaking down complex tasks into simpler ones and choosing the best tools for each sub-task. The performance of LLMs can be further boosted with external tool usages to be more accurate, efficient or versatile for wider applications.

### Approach

The researchers propose the use of tool documentation, which provides neutral and detailed descriptions of individual tool usage, as an alternative to demonstrations. They support their proposition with empirical evidence from six tasks across both vision and language modalities. Due to length constraints, a TF-IDF search is employed to retrieve the most relevant documentations. The study also demonstrates how to add new tools along with their documentation to a tool set for LLMs to solve unseen tasks on image editing and video tracking, all without any further demonstrations in a plug-and-play manner. The approach simplifies prompt design by only leveraging documentation for individual tools, while maintaining competitive performance. The study also includes an example of a new cloud service called LLMVM, which provides its own SDK CLI tool (llmcloud), demonstrating the practical application of the approach.
### Results

The study found that zero-shot prompts with only tool documentation were sufficient to elicit proper tool usage, matching or even surpassing the performance of few-shot prompts on existing benchmarks. On a newly collected realistic tool-use dataset with hundreds of available tool APIs, tool documentation was significantly more beneficial than demonstrations. The advantages of tool documentation were further highlighted by successfully tackling image generation and video tracking using just-released unseen state-of-the-art models as tools. Furthermore, the study showcases that LLMs are capable of re-inventing popular yet even more recent works Grounded-SAM and Track Anything, which suggests a potential from zero-shot tool usage to automatic knowledge discovery. The study also demonstrated the effectiveness of the approach in a tabular math reasoning task on the TabMWP dataset, where zero-shot prompting with tool documentation outperformed several models specifically finetuned on the dataset, even surpassing the performance of few-shot methods without any demonstrations. In the NLVRv2 dataset, zero-shot prompting with tool documentation achieved decent performance compared to only using few-shot demos, indicating that the few-shot demos may require careful curation for the model to achieve good performance. The results also showed that the model performance was stable with tool documentation, even as the number of demonstrations was reduced. In fact, on certain datasets, the model performed better with documentation alone than with additional demonstrations. An additional finding was that the performance of the LLM improved as the length of the documentation increased, up to a length of 600 words, indicating the value of comprehensive documentation. However, a degradation in performance was noted after the document length exceeds 600 words, attributed to the inherent challenges associated with comprehending lengthy documents in language models. The results on the LLM-Cloud CLI with different underlying LLM planners showed that documentation is much more important than few-shot demonstrations, especially when there is a large number of tools. The study also demonstrated that with tool docs, image editing examples achieved by VisProg could be reproduced without using any few-shot demos.
## Conclusion

The study concludes that tool documentation can be used to automatically enable new applications. By using only the documentation of GroundingDino, Stable Diffusion, XMem, and SAM, LLMs can re-invent the functionalities of the just-released Grounded-SAM and Track Anything models. This suggests that tool documentation could be a more effective and efficient method for teaching LLMs new tools. The paper underscores the importance of understanding the capabilities of each tool, akin to a craftsman knowing what their tools can do. The approach simplifies the process and eliminates the need for additional training or human supervision, making it more practical for plug-and-play usage. The study also highlights the potential of tool documentation in reducing the reliance on demonstrations, thereby making the process of teaching LLMs more scalable, especially when dealing with a large number of tools. The study also found that the quality of documentation, as indicated by its length, has a significant impact on the performance of the LLM, further emphasizing the importance of comprehensive documentation. The study acknowledges the limitation of LLMs in handling lengthy documents and suggests that ongoing advancements in language models will gradually address this issue. The exploration of solutions to overcome this limitation is left for future research.
