# Tool Learning with Foundation Models

**Relevance: ...**

**Link**:
- Paper: [Arxiv](https://arxiv.org/pdf/2304.08354.pdf)
- Code: N/A

**Authors**: Yujia Qin, Shengding Hu, Yankai Lin _and 40 others_

## Summary

**TL;DR: This paper presents a comprehensive review of tool learning with foundation models, introducing the concept of 'intoolligence', a taxonomy for classifying tools, and a unified tool learning framework, while demonstrating the potential of AI in scientific discovery and addressing challenges such as system learning, privacy concerns, and knowledge conflicts, with experiments showing the proficiency of current models in using tools and advocating for further research in integrating tools with foundation models.**

The paper provides a comprehensive review of tool learning with foundation models, comparing human tool use with AI's potential capabilities. It introduces the concept of 'intoolligence', dividing tool use behavior into three modes: assistive, arbitrary, and free tool use, each representing a progressive level of intelligence. The paper also introduces a taxonomy for classifying tools based on their modes of expression and interaction, ranging from physical to graphical user interface (GUI) to program level interactions. The authors argue that regardless of the tool type, it is fundamentally possible to leverage foundation models to execute them by setting up intermediary interfaces. The paper also emphasizes the importance of generalization and abstraction in tool learning, drawing parallels with human intelligence. Furthermore, the concept of 'digital embodiment' is introduced, where an AI agent interacts with the world through tools, akin to a form of embodiment. The paper also highlights the potential of AI in scientific discovery, demonstrating its effectiveness in various scientific scenarios and the potential for AI to manipulate scientific tools and solve multidisciplinary problems.

### Approach

The paper proposes a unified tool learning framework, which includes four fundamental components: tool set, environment, controller, and perceiver. The controller, serving as the 'brain' of the framework, is typically modeled using a foundation model. It is responsible for understanding user intent, the relationship between the intent and available tools, and developing a plan to select the appropriate tools for tasks. The controller also integrates the execution results into a plausible response for the user. For complex queries targeting high-level tasks, the controller decomposes the task into multiple sub-tasks, requiring powerful planning and reasoning capabilities from the foundation models. The paper also introduces the concept of a unified interface for tool learning, which serves as the foundation for generalizable tool learning. Through this unified interface, models can identify and abstract essential features of tools more easily, facilitating knowledge transfer among tools. The paper further discusses three potential ways of interface unification: the semantic interface, GUI interface, and programming interface. The authors also highlight the importance of building a tool library for foundation models, citing emerging works such as LangChain, TaskMatrix.AI, HuggingGPT, and ChatGPT Plugins. The paper also discusses the challenges of system learning, where foundation models need to acquire and comprehend the knowledge and functions of large complex systems. The authors propose a three-step approach to address this issue: fine-tuning foundation models with textual materials or source code, designing prompts to assist foundation models in using specific functions, and augmenting these models with a checking layer to ensure accurate generation of function calls. The paper also introduces strategies to improve the efficiency of tool learning, such as filtering out irrelevant information, adding hints in zero-prompting, and caching relevant knowledge in high-speed storage. In addition, the paper addresses privacy concerns in tool learning, proposing federated learning and model distillation as potential solutions to ensure data privacy and security. The paper also introduces the concept of personalized tool learning, emphasizing the importance of considering user-specific information in tool manipulation. The authors discuss the challenges of heterogeneous user information modeling and personalized tool planning and call, and propose solutions to address these issues. The paper also discusses the potential conflicts that may arise when the controller retrieves knowledge from multiple tools, due to differences in credibility, biases, and algorithmic implementation of the tools. The authors suggest that models should have the ability to distinguish and verify the reliability of various sources. To achieve this goal, they propose two research directions: conflict detection and conflict resolution.

### Results

Experiments with 18 representative tools demonstrate the potential of current foundation models in proficiently using tools. It is shown that state-of-the-art foundation models, such as ChatGPT, can effectively use tools to solve tasks with simple prompting. However, the use of tools with zero-shot prompting sometimes leads to worse performance, indicating that sub-optimal utilization of tools may negatively impact performance. Nevertheless, incorporating tools with few-shot prompting consistently yields superior performance. The paper also identifies several open problems, including ensuring safe and trustworthy tool use, enabling tool creation with foundation models, and addressing personalization challenges. The potential of physical interaction-based tools, GUI-based tools, and program-based tools to improve efficiency and productivity in various applications is also highlighted. The effectiveness of both zero-shot and few-shot prompting methods is also demonstrated, though challenges remain, particularly for smaller or less capable models and when restricted by input context length. The paper also discusses the challenges faced when the tool’s output is not aligned with the model’s input format and presents solutions such as chaining together foundation models of various modalities by converting their outputs into natural languages or building multimodal foundation models that can perceive general modalities. Additional experiments were conducted using a variety of tools, including movie search APIs, AI painting tools, 3D model construction tools, and chemical property query tools, demonstrating the model's proficiency in a wide range of tasks and domains. Furthermore, the paper explores the potential of tool learning in accessing database data via natural language, with a focus on complex queries under the TPC-H schema. The evaluation metric is the ratio of queries for which tool learning can accurately output the results. The performance of different tools under few-shot prompting setting is also discussed, with tools such as Map, Weather, Slides, Tables, Cooking Assistant, and AI Painting exhibiting a satisfying completion rate. However, for several tools such as KGs, Wikipedia, online shopping, and 3D model construction, the model performance is still far from satisfactory even with few-shot prompting.

### Conclusion

The paper advocates for further research in integrating tools with foundation models, emphasizing the evolution of AI from tool user to tool maker, the transition from general to personalized intelligence, and the relationship between tool learning and embodied learning. The authors also discuss important research topics for real-world application, including safety and trustworthiness, tool learning for large complex systems, tool creation, personalized tool learning, embodied learning, knowledge conflicts in tool augmentation, and other open problems. The authors have made the relevant codes and datasets publicly accessible for further research exploration. The integration of specialized tools and foundation models represents a promising approach for harnessing the unique strengths of both. The paper also highlights the importance of extrospective reasoning, which requires interaction between the controller and the environment, leading to a more comprehensive understanding of the current situation and enabling the achievement of long-term goals that necessitate extensive planning. The authors also introduce BMTools, an open-source repository that extends foundation models using tools and serves as a platform for the community to build and share tools. The paper further discusses the safety and trustworthiness problems of tool learning, highlighting the potential threats from external adversaries and the need for models to not only learn to use tools, but also possess the ability to scrutinize, rectify, and secure them. The paper also emphasizes the need for governance over foundation models, particularly in the context of tool learning. It raises concerns about the potential misuse of AI and the need for careful deliberation over which tools should be mastered by models. The paper also highlights the need for models to follow regulations and constraints of tool usage. The paper further discusses the trustworthiness of these models, highlighting the challenges of determining their reliability, especially in high-stake scenarios such as autonomous driving and clinical trials. The paper also raises concerns about the morality of foundation models, emphasizing the need for these models to be mild and compliant, and the challenges of controlling their actions when they learn actively from the world via tools. The paper also discusses the shift from reactive to proactive systems in the context of tool learning, highlighting the opportunities and challenges this paradigm shift presents. Proactive systems can continually improve their performance and tailor their responses to specific users, providing a more personalized and seamless user experience. However, they can also initiate actions that have unintended consequences, particularly in complex and dynamic environments, leading to cascading failures that are difficult to control. The paper also emphasizes the importance of privacy-preserving technologies in personalized tool learning, highlighting the risks of data extraction attacks and the need for secure and trustworthy mechanisms to protect user privacy. The authors suggest exploring model-oriented distributed computing frameworks, such as edge computing and federated learning, to prevent data leakage. The paper also raises an intriguing question about the future development of foundation models for tool learning: should the capabilities of these models be primarily internalized, or should they rely more heavily on external tools? This question is posed in light of recent advances in foundation models that exhibit contrasting trends, and the potential implications and trade-offs of these trends are discussed. The paper also suggests that tool learning performance can serve as a next-generation gauge for measuring machine intelligence, offering several advantages over traditional evaluation metrics. Tool use evaluation requires AI systems to go beyond memorization and use their acquired knowledge to accomplish specific tasks, which better aligns with real-world applications and the notion of practical intelligence. The paper also introduces the ethical considerations of integrating foundation models with human labor, highlighting potential conflicts with ethical principles and the potential for devaluing human dignity and commodifying human expertise. To address these ethical concerns, it is essential for the community to establish guidelines and safeguards that prioritize human dignity and agency when integrating human labor with foundation models. This may involve setting clear boundaries on the types of tasks that can be delegated to humans, ensuring fair compensation and working conditions, and promoting transparency in the development of AI systems. Moreover, fostering collaboration between AI researchers, ethicists, policymakers, and other stakeholders is crucial to develop a comprehensive understanding of the ethical implications of human-model collaboration and to create effective regulations that safeguard human rights and dignity. The paper also acknowledges the challenges of realizing these ideas, particularly the safety issues of accessing physical tools and the heterogeneity of scientific data. It suggests exploring how to fuse the general intelligence learned from plain text and the expertise needed for scientific discovery. The paper cites the work of Boiko et al. (2023) as an example of the potential of this direction, where a system uses foundation models to design, plan, and execute scientific experiments.
