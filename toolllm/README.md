<!--- Created using: gpt-4 --->
<!--- Reviewed: False --->
# ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs

**Link**: [Paper](http://arxiv.org/pdf/2307.16789v1)
**Authors**: Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, Maosong Sun

## Summary

**TL;DR: The study introduces ToolLLM and ToolBench, a framework and dataset respectively, aimed at enhancing the tool-use capabilities of large language models (LLMs), with the developed ToolLLaMA demonstrating robust performance in executing complex instructions and generalizing to unseen APIs, thus contributing to the democratization of AI technologies.**

The study focuses on tool learning, a process that enables large language models (LLMs) to interact effectively with various tools (APIs) to accomplish complex tasks. Despite the versatility of open-source LLMs like LLaMA and Vicuna, they lack the sophistication required for higher-level tasks, such as understanding human instructions and interacting appropriately with tools. This is due to the focus of instruction tuning on basic language tasks rather than the tool-use domain. The researchers aim to address this gap and democratize AI technologies by empowering open-source LLMs to master diverse APIs.

### Approach

The researchers introduced ToolLLM, a general tool-use framework, and ToolBench, an instruction-tuning dataset for tool use. ToolBench was created using the latest version of ChatGPT to generate diverse human instructions involving 16,464 real-world RESTful APIs from RapidAPI Hub. The construction of ToolBench involved three phases, including API collection, instruction generation, and solution path annotation. The researchers also developed a depth-first search-based decision tree (DFSDT) to bolster the planning and reasoning ability of LLMs, enabling them to evaluate multiple reasoning paths and make deliberate decisions. The DFSDT allows the model to assess different reasoning paths and choose to either proceed along a promising path or abandon an existing node. During node expansion, to diversify the child nodes and expand the search space, the model is prompted with the information of the previously generated nodes and explicitly encouraged to generate a distinct node. Additionally, they trained a neural API retriever using ChatGPT's recommendations for relevant APIs, which was integrated with ToolLLaMA to enhance its decision-making process. The APIs were carefully selected and filtered based on functionality, response time, and quality. Furthermore, a response compression strategy was implemented to handle lengthy API responses, ensuring the critical information is retained while fitting within the context length limitations of LLMs. The instruction generation process was designed to ensure diversity and multi-tool usage, mirroring real-world scenarios and boosting the generalizability and robustness of LLMs. The researchers also leveraged the function call feature of gpt-3.5-turbo-16k, treating each API as a special function and feeding its API documentation into the ChatGPT’s function field. This allowed the model to understand how to call the API and expanded the action space. To improve parameter efficiency, a representative parameter-efficient tuning method, LoRA, was applied.
### Results

The researchers fine-tuned LLaMA on ToolBench to create ToolLLaMA. Their automatic evaluator, ToolEval, showed that ToolLLaMA could execute complex instructions and generalize to unseen APIs, exhibiting comparable performance to ChatGPT. ToolEval, backed up by ChatGPT, comprises two key metrics: pass rate and win rate. In tool-use evaluation, ToolLLaMA demonstrated a high pass rate and win rate, surpassing Text-Davinci-003 and almost performing on par with ChatGPT. Notably, ToolLLaMA showed robust generalization to previously unseen APIs and could handle both single-tool and complex multi-tool instructions. The API retriever exhibited remarkable retrieval precision, returning APIs closely aligned with the ground truth. The researchers also highlighted the importance of balancing effectiveness with costs, and thus, they adopted a pre-order traversal variant for DFSDT, which achieved similar performance as DFS while significantly reducing costs. The ChatGPT evaluator demonstrated a high correlation of 75.8% with human annotators, indicating its reliability and consistency in evaluation. The API retriever showed high NDCG scores across different instruction categories, indicating its effectiveness in retrieving relevant APIs. Furthermore, the API retriever consistently outperformed BM25 and Ada Embedding across different types of instructions, with the highest NDCG score for single-tool instructions, indicating that these are relatively simpler for API retrieval than multi-tool instructions. In a comparison between DFSDT and ReACT, DFSDT significantly outperformed the two baselines in all scenarios, showing that DFSDT is a more efficient way that saves the costs for solution path annotation. The performance improvement of DFSDT was more evident for harder instructions than simpler instructions, indicating that DFSDT can solve those difficult, complex instructions that are unanswerable by the vanilla ReACT no matter how many times it is performed. In practice, users may not be able to manually recommend APIs from a large pool. To emulate this practical setting and test the efficiency of the API retriever, the researchers replaced the ground truth APIs with the top 5 APIs recommended by the API retriever. The results showed that ToolLLaMA significantly outperformed the conventional method for tool use, i.e., ChatGPT-ReACT, in both pass rate and win rate, exhibiting superior generalization abilities. Besides, ToolLLaMA also performed better than Text-Dainci-003 when combined with DFSDT. Although extensive prompt engineering was conducted, both Vicuna and Alpaca failed to pass any instructions, underscoring the deficiency of current instruction tuning methods, which largely focus on enhancing language skills. In general, ToolLLaMA demonstrated competitive performance in all scenarios, achieving a pass rate slightly lower than ChatGPT+DFSDT. For the win rate, ToolLLaMA generally matched ChatGPT+DFSDT’s capability and even surpassed the latter in the I2-Cat. setting.
## Conclusion

The study presents a significant advancement in enhancing the tool-use capabilities of open-source LLMs. The researchers have made the codes, trained models, and demo publicly available, paving the way for further research and practical applications in this domain. This work contributes to the democratization of AI technologies and encourages community-driven innovation and development. The researchers hope that this work will inspire further research in the intersection of instruction tuning and tool use. However, the study also acknowledges that improved parameter efficiency was achieved with a trade-off in performance, and future attempts should aim to design more sophisticated methods that could achieve parameter efficiency without sacrificing performance. The researchers' DFSDT extends previous work by allowing LLMs to assess different reasoning paths and select the most promising one, addressing general decision-making problems where the decision space is infinite. This method is designed for diverse decision-making tasks, unlike previous methods tailored specifically for selected task sets.
